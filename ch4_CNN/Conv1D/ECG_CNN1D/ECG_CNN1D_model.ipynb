{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8619b1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Conv1D, Dropout, MaxPool1D, Flatten\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ef04ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is not a mandatory step\n",
    "# This is used to set the figure size so that the plots appear to be of that size\n",
    "# If this parameter is not set, then the default plot size is used\n",
    "plt.rcParams['figure.figsize'] = 30, 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1b18d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all the preformatted datasets fromm the saved location\n",
    "X_train = np.load('X_train.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "y_test = np.load('y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166105e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step is actually supposed to be used to randomly split dataset into training and testing set as well as to shuffle them\n",
    "# However, here I am using it just to shuffle the dataset and therefore the test_size is equated to 0\n",
    "X_train, _, y_train, _ = train_test_split(X_train, y_train, test_size=0, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dffa6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just printing out the dimentions to verify the data\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23545175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "\n",
    "# The model architecture type is sequential hence that is used\n",
    "model = Sequential()\n",
    "\n",
    "# We are using 4 convolution layers for feature extraction\n",
    "model.add(Conv1D(filters=512, kernel_size=32, padding='same', kernel_initializer='normal', activation='relu', input_shape=(256, 2)))\n",
    "model.add(Conv1D(filters=512, kernel_size=32, padding='same', kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dropout(0.2)) # This is the dropout layer. It's main function is to inactivate 20% of neurons in order to prevent overfitting\n",
    "model.add(Conv1D(filters=256, kernel_size=32, padding='same', kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(filters=256, kernel_size=32, padding='same', kernel_initializer='normal', activation='relu'))\n",
    "model.add(MaxPool1D(pool_size=128)) # We use MaxPooling with a filter size of 128. This also contributes to generalization\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# The prevous step gices an output of multi dimentional data, which cannot be fead directly into the feed forward neural network. Hence, the model is flattened\n",
    "model.add(Flatten())\n",
    "# One hidden layer of 128 neurons have been used in order to have better classification results\n",
    "model.add(Dense(units=128, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "# The final neuron HAS to be 1 in number and cannot be more than that. This is because this is a binary classification problem and only 1 neuron is enough to denote the class '1' or '0'\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Print the summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc6d995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the mode\n",
    "# To my experience, the Stocastic Gradient Descent Optimizer works the best. Adam optimizer also works but not as good as SGD\n",
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.5)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train,  batch_size=64, epochs=5, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaed1221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model Accuracy graph (Ideally, it should be Logarithmic shape)\n",
    "plt.plot(history.history['acc'],'r',linewidth=3.0, label='Training Accuracy')\n",
    "plt.plot(history.history['val_acc'],'b',linewidth=3.0, label='Testing Accuracy')\n",
    "plt.legend(fontsize=18)\n",
    "plt.xlabel('Epochs ', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "plt.title('Accuracy Curves', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb7e1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model Loss graph (Ideally it should be Exponentially decreasing shape)\n",
    "plt.plot(history.history['loss'], 'g', linewidth=3.0, label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], 'y', linewidth=3.0, label='Testing Loss')\n",
    "plt.legend(fontsize=18)\n",
    "plt.xlabel('Epochs ', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.title('Loss Curves', fontsize=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
