{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "231750c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compatibility layer between Python 2 and Python 3\n",
    "from __future__ import print_function\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Reshape, GlobalAveragePooling1D\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcae615e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feature_normalize(dataset):\n",
    "\n",
    "    mu = np.mean(dataset, axis=0)\n",
    "    sigma = np.std(dataset, axis=0)\n",
    "    return (dataset - mu)/sigma\n",
    "\n",
    "\n",
    "def show_confusion_matrix(validations, predictions):\n",
    "\n",
    "    matrix = metrics.confusion_matrix(validations, predictions)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(matrix,\n",
    "                cmap=\"coolwarm\",\n",
    "                linecolor='white',\n",
    "                linewidths=1,\n",
    "                xticklabels=LABELS,\n",
    "                yticklabels=LABELS,\n",
    "                annot=True,\n",
    "                fmt=\"d\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_basic_dataframe_info(dataframe,\n",
    "                              preview_rows=20):\n",
    "\n",
    "    \"\"\"\n",
    "    This function shows basic information for the given dataframe\n",
    "    Args:\n",
    "        dataframe: A Pandas DataFrame expected to contain data\n",
    "        preview_rows: An integer value of how many rows to preview\n",
    "    Returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "\n",
    "    # Shape and how many rows and columns\n",
    "    print(\"Number of columns in the dataframe: %i\" % (dataframe.shape[1]))\n",
    "    print(\"Number of rows in the dataframe: %i\\n\" % (dataframe.shape[0]))\n",
    "    print(\"First 20 rows of the dataframe:\\n\")\n",
    "    # Show first 20 rows\n",
    "    print(dataframe.head(preview_rows))\n",
    "    print(\"\\nDescription of dataframe:\\n\")\n",
    "    # Describe dataset like mean, min, max, etc.\n",
    "    # print(dataframe.describe())\n",
    "\n",
    "\n",
    "def read_data(file_path):\n",
    "\n",
    "    \"\"\"\n",
    "    This function reads the accelerometer data from a file\n",
    "    Args:\n",
    "        file_path: URL pointing to the CSV file\n",
    "    Returns:\n",
    "        A pandas dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    column_names = ['user-id',\n",
    "                    'activity',\n",
    "                    'timestamp',\n",
    "                    'x-axis',\n",
    "                    'y-axis',\n",
    "                    'z-axis']\n",
    "    df = pd.read_csv(file_path,\n",
    "                     header=None,\n",
    "                     names=column_names,\n",
    "                    sep=',')\n",
    "    # Last column has a \";\" character which must be removed ...\n",
    "    df['z-axis'].replace(regex=True,\n",
    "                         inplace=True,\n",
    "                         to_replace=r';',\n",
    "                         value=r'')\n",
    "    # ... and then this column must be transformed to float explicitly\n",
    "    df['z-axis'] = df['z-axis'].apply(convert_to_float)\n",
    "    # This is very important otherwise the model will not fit and loss\n",
    "    # will show up as NAN\n",
    "    df.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_to_float(x):\n",
    "\n",
    "    try:\n",
    "        return np.float(x)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "# Not used right now\n",
    "def feature_normalize(dataset):\n",
    "\n",
    "    mu = np.mean(dataset, axis=0)\n",
    "    sigma = np.std(dataset, axis=0)\n",
    "    return (dataset - mu)/sigma\n",
    "\n",
    "\n",
    "def plot_axis(ax, x, y, title):\n",
    "\n",
    "    ax.plot(x, y)\n",
    "    ax.set_title(title)\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.set_ylim([min(y) - np.std(y), max(y) + np.std(y)])\n",
    "    ax.set_xlim([min(x), max(x)])\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "def plot_activity(activity, data):\n",
    "\n",
    "    fig, (ax0, ax1, ax2) = plt.subplots(nrows=3,\n",
    "         figsize=(15, 10),\n",
    "         sharex=True)\n",
    "    plot_axis(ax0, data['timestamp'], data['x-axis'], 'x-axis')\n",
    "    plot_axis(ax1, data['timestamp'], data['y-axis'], 'y-axis')\n",
    "    plot_axis(ax2, data['timestamp'], data['z-axis'], 'z-axis')\n",
    "    plt.subplots_adjust(hspace=0.2)\n",
    "    fig.suptitle(activity)\n",
    "    plt.subplots_adjust(top=0.90)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "343cbbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_segments_and_labels(df, time_steps, step, label_name):\n",
    "\n",
    "    \"\"\"\n",
    "    This function receives a dataframe and returns the reshaped segments\n",
    "    of x,y,z acceleration as well as the corresponding labels\n",
    "    Args:\n",
    "        df: Dataframe in the expected format\n",
    "        time_steps: Integer value of the length of a segment that is created\n",
    "    Returns:\n",
    "        reshaped_segments\n",
    "        labels:\n",
    "    \"\"\"\n",
    "\n",
    "    # x, y, z acceleration as features\n",
    "    N_FEATURES = 3\n",
    "    # Number of steps to advance in each iteration (for me, it should always\n",
    "    # be equal to the time_steps in order to have no overlap between segments)\n",
    "    # step = time_steps\n",
    "    segments = []\n",
    "    labels = []\n",
    "    for i in range(0, len(df) - time_steps, step):\n",
    "        xs = df['x-axis'].values[i: i + time_steps]\n",
    "        ys = df['y-axis'].values[i: i + time_steps]\n",
    "        zs = df['z-axis'].values[i: i + time_steps]\n",
    "        # Retrieve the most often used label in this segment\n",
    "        label = stats.mode(df[label_name][i: i + time_steps])[0][0]\n",
    "        segments.append([xs, ys, zs])\n",
    "        labels.append(label)\n",
    "\n",
    "    # Bring the segments into a better shape\n",
    "    reshaped_segments = np.asarray(segments, dtype= np.float32).reshape(-1, time_steps, N_FEATURES)\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    return reshaped_segments, labels\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e95a5d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras version  2.8.0\n",
      "\n",
      "--- Load, inspect and transform data ---\n",
      "\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 6 fields in line 134634, saw 11\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Load, inspect and transform data ---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Load data set containing all the data from csv\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m##df = read_data('./WISDM_ar_v1.1_raw.txt')\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mread_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./WISDM_ar_v1.1_raw.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Describe the data\u001b[39;00m\n\u001b[1;32m     30\u001b[0m show_basic_dataframe_info(df, \u001b[38;5;241m20\u001b[39m)\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mread_data\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03mThis function reads the accelerometer data from a file\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    A pandas dataframe\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m column_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser-id\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     60\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivity\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     61\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     62\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx-axis\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     63\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my-axis\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     64\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz-axis\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 65\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Last column has a \";\" character which must be removed ...\u001b[39;00m\n\u001b[1;32m     69\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz-axis\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     70\u001b[0m                      inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     71\u001b[0m                      to_replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     72\u001b[0m                      value\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py:581\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1250\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1248\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1250\u001b[0m     index, columns, col_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:225\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 225\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    227\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:805\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:847\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:1960\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 6 fields in line 134634, saw 11\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ------- THE PROGRAM TO LOAD DATA AND TRAIN THE MODEL -------\n",
    "\n",
    "# Set some standard parameters upfront\n",
    "pd.options.display.float_format = '{:.1f}'.format\n",
    "sns.set() # Default seaborn look and feel\n",
    "plt.style.use('ggplot')\n",
    "print('keras version ', keras.__version__)\n",
    "\n",
    "LABELS = [\"Downstairs\",\n",
    "          \"Jogging\",\n",
    "          \"Sitting\",\n",
    "          \"Standing\",\n",
    "          \"Upstairs\",\n",
    "          \"Walking\"]\n",
    "# The number of steps within one time segment\n",
    "TIME_PERIODS = 80\n",
    "# The steps to take from one segment to the next; if this value is equal to\n",
    "# TIME_PERIODS, then there is no overlap between the segments\n",
    "STEP_DISTANCE = 40\n",
    "\n",
    "# %%\n",
    "\n",
    "print(\"\\n--- Load, inspect and transform data ---\\n\")\n",
    "\n",
    "# Load data set containing all the data from csv\n",
    "##df = read_data('./WISDM_ar_v1.1_raw.txt')\n",
    "df = read_data('./WISDM_ar_v1.1_raw.txt')\n",
    "\n",
    "# Describe the data\n",
    "show_basic_dataframe_info(df, 20)\n",
    "\n",
    "df['activity'].value_counts().plot(kind='bar',\n",
    "                                   title='Training Examples by Activity Type')\n",
    "plt.show()\n",
    "\n",
    "df['user-id'].value_counts().plot(kind='bar',\n",
    "                                  title='Training Examples by User')\n",
    "plt.show()\n",
    "\n",
    "for activity in np.unique(df[\"activity\"]):\n",
    "    subset = df[df[\"activity\"] == activity][:180]\n",
    "    plot_activity(activity, subset)\n",
    "\n",
    "# Define column name of the label vector\n",
    "LABEL = \"ActivityEncoded\"\n",
    "# Transform the labels from String to Integer via LabelEncoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "# Add a new column to the existing DataFrame with the encoded values\n",
    "df[LABEL] = le.fit_transform(df[\"activity\"].values.ravel())\n",
    "\n",
    "# %%\n",
    "\n",
    "print(\"\\n--- Reshape the data into segments ---\\n\")\n",
    "\n",
    "# Differentiate between test set and training set\n",
    "df_test = df[df['user-id'] > 28]\n",
    "df_train = df[df['user-id'] <= 28]\n",
    "\n",
    "# Normalize features for training data set\n",
    "df_train['x-axis'] = feature_normalize(df['x-axis'])\n",
    "df_train['y-axis'] = feature_normalize(df['y-axis'])\n",
    "df_train['z-axis'] = feature_normalize(df['z-axis'])\n",
    "# Round in order to comply to NSNumber from iOS\n",
    "df_train = df_train.round({'x-axis': 6, 'y-axis': 6, 'z-axis': 6})\n",
    "\n",
    "# Reshape the training data into segments\n",
    "# so that they can be processed by the network\n",
    "x_train, y_train = create_segments_and_labels(df_train,\n",
    "                                              TIME_PERIODS,\n",
    "                                              STEP_DISTANCE,\n",
    "                                              LABEL)\n",
    "\n",
    "# %%\n",
    "\n",
    "print(\"\\n--- Reshape data to be accepted by Keras ---\\n\")\n",
    "\n",
    "# Inspect x data\n",
    "print('x_train shape: ', x_train.shape)\n",
    "# Displays (20869, 40, 3)\n",
    "print(x_train.shape[0], 'training samples')\n",
    "# Displays 20869 train samples\n",
    "\n",
    "# Inspect y data\n",
    "print('y_train shape: ', y_train.shape)\n",
    "# Displays (20869,)\n",
    "\n",
    "# Set input & output dimensions\n",
    "num_time_periods, num_sensors = x_train.shape[1], x_train.shape[2]\n",
    "num_classes = le.classes_.size\n",
    "print(list(le.classes_))\n",
    "\n",
    "# Set input_shape / reshape for Keras\n",
    "# Remark: acceleration data is concatenated in one array in order to feed\n",
    "# it properly into coreml later, the preferred matrix of shape [40,3]\n",
    "# cannot be read in with the current version of coreml (see also reshape\n",
    "# layer as the first layer in the keras model)\n",
    "input_shape = (num_time_periods*num_sensors)\n",
    "x_train = x_train.reshape(x_train.shape[0], input_shape)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "# x_train shape: (20869, 120)\n",
    "print('input_shape:', input_shape)\n",
    "# input_shape: (120)\n",
    "\n",
    "# Convert type for Keras otherwise Keras cannot process the data\n",
    "x_train = x_train.astype(\"float32\")\n",
    "y_train = y_train.astype(\"float32\")\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11bc53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding of y_train labels (only execute once!)\n",
    "y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "print('New y_train shape: ', y_train.shape)\n",
    "# (4173, 6)\n",
    "\n",
    "# %%\n",
    "\n",
    "print(\"\\n--- Create neural network model ---\\n\")\n",
    "\n",
    "# 1D CNN neural network\n",
    "model_m = Sequential()\n",
    "model_m.add(Reshape((TIME_PERIODS, num_sensors), input_shape=(input_shape,)))\n",
    "model_m.add(Conv1D(100, 10, activation='relu', input_shape=(TIME_PERIODS, num_sensors)))\n",
    "model_m.add(Conv1D(100, 10, activation='relu'))\n",
    "model_m.add(MaxPooling1D(3))\n",
    "model_m.add(Conv1D(160, 10, activation='relu'))\n",
    "model_m.add(Conv1D(160, 10, activation='relu'))\n",
    "model_m.add(GlobalAveragePooling1D())\n",
    "model_m.add(Dropout(0.5))\n",
    "model_m.add(Dense(num_classes, activation='softmax'))\n",
    "print(model_m.summary())\n",
    "# Accuracy on training data: 99%\n",
    "# Accuracy on test data: 91%\n",
    "\n",
    "# %%\n",
    "\n",
    "print(\"\\n--- Fit the model ---\\n\")\n",
    "\n",
    "# The EarlyStopping callback monitors training accuracy:\n",
    "# if it fails to improve for two consecutive epochs,\n",
    "# training stops early\n",
    "callbacks_list = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='best_model.{epoch:02d}-{val_loss:.2f}.h5',\n",
    "        monitor='val_loss', save_best_only=True),\n",
    "    keras.callbacks.EarlyStopping(monitor='acc', patience=1)\n",
    "]\n",
    "\n",
    "model_m.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Hyper-parameters\n",
    "BATCH_SIZE = 400\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94853428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable validation to use ModelCheckpoint and EarlyStopping callbacks.\n",
    "history = model_m.fit(x_train,\n",
    "                      y_train,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      epochs=EPOCHS,\n",
    "                      callbacks=callbacks_list,\n",
    "                      validation_split=0.2,\n",
    "                      verbose=1)\n",
    "\n",
    "# %%\n",
    "\n",
    "print(\"\\n--- Learning curve of model training ---\\n\")\n",
    "\n",
    "# summarize history for accuracy and loss\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(history.history['acc'], \"g--\", label=\"Accuracy of training data\")\n",
    "plt.plot(history.history['val_acc'], \"g\", label=\"Accuracy of validation data\")\n",
    "plt.plot(history.history['loss'], \"r--\", label=\"Loss of training data\")\n",
    "plt.plot(history.history['val_loss'], \"r\", label=\"Loss of validation data\")\n",
    "plt.title('Model Accuracy and Loss')\n",
    "plt.ylabel('Accuracy and Loss')\n",
    "plt.xlabel('Training Epoch')\n",
    "plt.ylim(0)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#%%\n",
    "\n",
    "print(\"\\n--- Check against test data ---\\n\")\n",
    "\n",
    "# Normalize features for training data set\n",
    "df_test['x-axis'] = feature_normalize(df_test['x-axis'])\n",
    "df_test['y-axis'] = feature_normalize(df_test['y-axis'])\n",
    "df_test['z-axis'] = feature_normalize(df_test['z-axis'])\n",
    "\n",
    "df_test = df_test.round({'x-axis': 6, 'y-axis': 6, 'z-axis': 6})\n",
    "\n",
    "x_test, y_test = create_segments_and_labels(df_test,\n",
    "                                            TIME_PERIODS,\n",
    "                                            STEP_DISTANCE,\n",
    "                                            LABEL)\n",
    "\n",
    "# Set input_shape / reshape for Keras\n",
    "x_test = x_test.reshape(x_test.shape[0], input_shape)\n",
    "\n",
    "x_test = x_test.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")\n",
    "\n",
    "y_test = np_utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "score = model_m.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "print(\"\\nAccuracy on test data: %0.2f\" % score[1])\n",
    "print(\"\\nLoss on test data: %0.2f\" % score[0])\n",
    "\n",
    "# %%\n",
    "\n",
    "print(\"\\n--- Confusion matrix for test data ---\\n\")\n",
    "\n",
    "y_pred_test = model_m.predict(x_test)\n",
    "# Take the class with the highest probability from the test predictions\n",
    "max_y_pred_test = np.argmax(y_pred_test, axis=1)\n",
    "max_y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "show_confusion_matrix(max_y_test, max_y_pred_test)\n",
    "\n",
    "# %%\n",
    "\n",
    "print(\"\\n--- Classification report for test data ---\\n\")\n",
    "\n",
    "print(classification_report(max_y_test, max_y_pred_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
